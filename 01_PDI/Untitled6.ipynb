{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    \"\"\" Gaussian Mixture Model\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "        k: int , number of gaussian distributions\n",
    "        \n",
    "        seed: int, will be randomly set if None\n",
    "        \n",
    "        max_iter: int, number of iterations to run algorithm, default: 200\n",
    "        \n",
    "    Attributes\n",
    "    -----------\n",
    "       centroids: array, k, number_features\n",
    "       \n",
    "       cluster_labels: label for each data point\n",
    "       \n",
    "    \"\"\"\n",
    "    def __init__(self, C, n_runs):\n",
    "        self.C = C # number of Guassians/clusters\n",
    "        self.n_runs = n_runs\n",
    "        \n",
    "    \n",
    "    def get_params(self):\n",
    "        return (self.mu, self.pi, self.sigma)\n",
    "    \n",
    "    \n",
    "        \n",
    "    def calculate_mean_covariance(self, X, prediction):\n",
    "        \"\"\"Calculate means and covariance of different\n",
    "            clusters from k-means prediction\n",
    "        \n",
    "        Parameters:\n",
    "        ------------\n",
    "        prediction: cluster labels from k-means\n",
    "        \n",
    "        X: N*d numpy array data points \n",
    "        \n",
    "        Returns:\n",
    "        -------------\n",
    "        intial_means: for E-step of EM algorithm\n",
    "        \n",
    "        intial_cov: for E-step of EM algorithm\n",
    "        \n",
    "        \"\"\"\n",
    "        d = X.shape[1]\n",
    "        labels = np.unique(prediction)\n",
    "        self.initial_means = np.zeros((self.C, d))\n",
    "        self.initial_cov = np.zeros((self.C, d, d))\n",
    "        self.initial_pi = np.zeros(self.C)\n",
    "        \n",
    "        counter=0\n",
    "        for label in labels:\n",
    "            ids = np.where(prediction == label) # returns indices\n",
    "            self.initial_pi[counter] = len(ids[0]) / X.shape[0]\n",
    "            self.initial_means[counter,:] = np.mean(X[ids], axis = 0)\n",
    "            de_meaned = X[ids] - self.initial_means[counter,:]\n",
    "            Nk = X[ids].shape[0] # number of data points in current gaussian\n",
    "            self.initial_cov[counter,:, :] = np.dot(self.initial_pi[counter] * de_meaned.T, de_meaned) / Nk\n",
    "            counter+=1\n",
    "        assert np.sum(self.initial_pi) == 1    \n",
    "            \n",
    "        return (self.initial_means, self.initial_cov, self.initial_pi)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _initialise_parameters(self, X):\n",
    "        \"\"\"Implement k-means to find starting\n",
    "            parameter values.\n",
    "            https://datascience.stackexchange.com/questions/11487/how-do-i-obtain-the-weight-and-variance-of-a-k-means-cluster\n",
    "        Parameters:\n",
    "        ------------\n",
    "        X: numpy array of data points\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        tuple containing initial means and covariance\n",
    "        \n",
    "        _initial_means: numpy array: (C*d)\n",
    "        \n",
    "        _initial_cov: numpy array: (C,d*d)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        n_clusters = self.C\n",
    "        kmeans = KMeans(n_clusters= n_clusters, init=\"k-means++\", max_iter=500, algorithm = 'auto')\n",
    "        fitted = kmeans.fit(X)\n",
    "        prediction = kmeans.predict(X)\n",
    "        self._initial_means, self._initial_cov, self._initial_pi = self.calculate_mean_covariance(X, prediction)\n",
    "        \n",
    "        \n",
    "        return (self._initial_means, self._initial_cov, self._initial_pi)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def _e_step(self, X, pi, mu, sigma):\n",
    "        \"\"\"Performs E-step on GMM model\n",
    "        Parameters:\n",
    "        ------------\n",
    "        X: (N x d), data points, m: no of features\n",
    "        pi: (C), weights of mixture components\n",
    "        mu: (C x d), mixture component means\n",
    "        sigma: (C x d x d), mixture component covariance matrices\n",
    "        Returns:\n",
    "        ----------\n",
    "        gamma: (N x C), probabilities of clusters for objects\n",
    "        \"\"\"\n",
    "        N = X.shape[0] \n",
    "        self.gamma = np.zeros((N, self.C))\n",
    "\n",
    "        const_c = np.zeros(self.C)\n",
    "        \n",
    "        \n",
    "        self.mu = self.mu if self._initial_means is None else self._initial_means\n",
    "        self.pi = self.pi if self._initial_pi is None else self._initial_pi\n",
    "        self.sigma = self.sigma if self._initial_cov is None else self._initial_cov\n",
    "\n",
    "        for c in range(self.C):\n",
    "            # Posterior Distribution using Bayes Rule\n",
    "            self.gamma[:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])\n",
    "\n",
    "        # normalize across columns to make a valid probability\n",
    "        gamma_norm = np.sum(self.gamma, axis=1)[:,np.newaxis]\n",
    "        self.gamma /= gamma_norm\n",
    "\n",
    "        return self.gamma\n",
    "    \n",
    "    \n",
    "    def _m_step(self, X, gamma):\n",
    "        \"\"\"Performs M-step of the GMM\n",
    "        We need to update our priors, our means\n",
    "        and our covariance matrix.\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (N x d), data \n",
    "        gamma: (N x C), posterior distribution of lower bound \n",
    "        Returns:\n",
    "        ---------\n",
    "        pi: (C)\n",
    "        mu: (C x d)\n",
    "        sigma: (C x d x d)\n",
    "        \"\"\"\n",
    "        N = X.shape[0] # number of objects\n",
    "        C = self.gamma.shape[1] # number of clusters\n",
    "        d = X.shape[1] # dimension of each object\n",
    "\n",
    "        # responsibilities for each gaussian\n",
    "        self.pi = np.mean(self.gamma, axis = 0)\n",
    "\n",
    "        self.mu = np.dot(self.gamma.T, X) / np.sum(self.gamma, axis = 0)[:,np.newaxis]\n",
    "\n",
    "        for c in range(C):\n",
    "            x = X - self.mu[c, :] # (N x d)\n",
    "            \n",
    "            gamma_diag = np.diag(self.gamma[:,c])\n",
    "            x_mu = np.matrix(x)\n",
    "            gamma_diag = np.matrix(gamma_diag)\n",
    "\n",
    "            sigma_c = x.T * gamma_diag * x\n",
    "            self.sigma[c,:,:]=(sigma_c) / np.sum(self.gamma, axis = 0)[:,np.newaxis][c]\n",
    "\n",
    "        return self.pi, self.mu, self.sigma\n",
    "    \n",
    "    \n",
    "    def _compute_loss_function(self, X, pi, mu, sigma):\n",
    "        \"\"\"Computes lower bound loss function\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (N x d), data \n",
    "        \n",
    "        Returns:\n",
    "        ---------\n",
    "        pi: (C)\n",
    "        mu: (C x d)\n",
    "        sigma: (C x d x d)\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        C = self.gamma.shape[1]\n",
    "        self.loss = np.zeros((N, C))\n",
    "\n",
    "        for c in range(C):\n",
    "            dist = mvn(self.mu[c], self.sigma[c],allow_singular=True)\n",
    "            self.loss[:,c] = self.gamma[:,c] * (np.log(self.pi[c]+0.00001)+dist.logpdf(X)-np.log(self.gamma[:,c]+0.000001))\n",
    "        self.loss = np.sum(self.loss)\n",
    "        return self.loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute the E-step and M-step and\n",
    "            Calculates the lowerbound\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (N x d), data \n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        instance of GMM\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        d = X.shape[1]\n",
    "        self.mu, self.sigma, self.pi =  self._initialise_parameters(X)\n",
    "        \n",
    "        try:\n",
    "            for run in range(self.n_runs):  \n",
    "                self.gamma  = self._e_step(X, self.mu, self.pi, self.sigma)\n",
    "                self.pi, self.mu, self.sigma = self._m_step(X, self.gamma)\n",
    "                loss = self._compute_loss_function(X, self.pi, self.mu, self.sigma)\n",
    "                \n",
    "                if run % 10 == 0:\n",
    "                    print(\"Iteration: %d Loss: %0.6f\" %(run, loss))        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Returns predicted labels using Bayes Rule to\n",
    "        Calculate the posterior distribution\n",
    "        \n",
    "        Parameters:\n",
    "        -------------\n",
    "        X: ?*d numpy array\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        labels: predicted cluster based on \n",
    "        highest responsibility gamma.\n",
    "        \n",
    "        \"\"\"\n",
    "        labels = np.zeros((X.shape[0], self.C))\n",
    "        \n",
    "        for c in range(self.C):\n",
    "            labels [:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])\n",
    "        labels  = labels .argmax(1)\n",
    "        return labels \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Returns predicted labels\n",
    "        \n",
    "        Parameters:\n",
    "        -------------\n",
    "        X: N*d numpy array\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        labels: predicted cluster based on \n",
    "        highest responsibility gamma.\n",
    "        \n",
    "        \"\"\"\n",
    "        post_proba = np.zeros((X.shape[0], self.C))\n",
    "        \n",
    "        for c in range(self.C):\n",
    "            # Posterior Distribution using Bayes Rule, try and vectorise\n",
    "            post_proba[:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])\n",
    "    \n",
    "        return post_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
